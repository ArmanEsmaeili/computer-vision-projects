{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Markdown cell content starts -->\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "  body {\n",
    "    background: #0f172a;\n",
    "    color: #e5e7eb;\n",
    "    font-family: Inter, sans-serif;\n",
    "  }\n",
    "  .assignment {\n",
    "    max-width: 900px;\n",
    "    margin: auto;\n",
    "    padding: 20px;\n",
    "    background: #1f2937;\n",
    "    border-radius: 12px;\n",
    "    border: 1px solid #374151;\n",
    "  }\n",
    "  h1 {\n",
    "    color: #22d3ee;\n",
    "  }\n",
    "  h2 {\n",
    "    border-left: 4px solid #22d3ee;\n",
    "    padding-left: 8px;\n",
    "  }\n",
    "  table {\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "    margin-top: 12px;\n",
    "  }\n",
    "  th, td {\n",
    "    border: 1px solid #374151;\n",
    "    padding: 8px;\n",
    "    text-align: left;\n",
    "  }\n",
    "  th {\n",
    "    background: #111827;\n",
    "  }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"assignment\">\n",
    "  <h1>Computer Vision Assignment</h1>\n",
    "  <p><strong>Course:</strong> Computer Vision<br>\n",
    "     <strong>Deadline:</strong> XXXX<br>\n",
    "     <strong>Total Points:</strong> 100</p>\n",
    "\n",
    "  <h2>Objective</h2>\n",
    "  <ol>\n",
    "    <li>Use pretrained CNNs for classification, detection, segmentation</li>\n",
    "    <li>Perform preprocessing</li>\n",
    "    <li>Run inference on input images</li>\n",
    "    <li>Evaluate results</li>\n",
    "  </ol>\n",
    "\n",
    "  <h2>Grading Breakdown</h2>\n",
    "  <table>\n",
    "    <tr><th>Criterion</th><th>Description</th><th>Points</th></tr>\n",
    "    <tr><td>Correct preprocessing</td><td>Resizing, normalization, tensor conversion</td><td>20</td></tr>\n",
    "    <tr><td>Correct model usage</td><td>Proper pretrained model inference</td><td>20</td></tr>\n",
    "    <tr><td>Visualization quality</td><td>Clear side-by-side figures</td><td>20</td></tr>\n",
    "    <tr><td>Analysis and discussion</td><td>Interpretation and insights</td><td>40</td></tr>\n",
    "    <tr><td colspan=\"2\"><strong>Total</strong></td><td><strong>100</strong></td></tr>\n",
    "  </table>\n",
    "\n",
    "  <h4>Answer all questions colored in red, or complete all parts in \"#TO DO\" sections.</h4>\n",
    "\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "<!-- Markdown cell content ends -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q timm segment-anything ultralytics\n",
    "!pip install -q pillow matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import timm\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Show All Input Images\n",
    "<span style=\"color:red\">\n",
    "Plot all the images in a single figure.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Task 1 – Image Classification\n",
    "---\n",
    "In this task, you are to use pretrained **EfficientNet-B4** model from torchvision/timm with 1000 classes. \n",
    "\n",
    "<span style=\"color:red\">\n",
    "Explain the functionality of the model briefly and indicate the dataset it is trained on.\n",
    "</span>\n",
    "\n",
    "**Required preprocessing**:\n",
    "You are asked to design and apply a **12‑step preprocessing pipeline** to prepare real‑world images for classification with EfficientNet‑B4. \n",
    "\n",
    "<span style=\"color:red\"> For each step, briefly explain why it is useful.</span>\n",
    "\n",
    "\n",
    "### Steps to include:\n",
    "1. Convert to NumPy for OpenCV operations\n",
    "2. Apply Gaussian blur  \n",
    "3. Perform unsharp masking – Enhance edges and improve clarity\n",
    "4. Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)improve local contrast.\n",
    "5. Convert back to PIL format.\n",
    "6. Pad to square shape – Preserve aspect ratio before resizing.\n",
    "7. Resize with high‑quality antialiasing.\n",
    "8. Apply random rotation and random crop.\n",
    "9. Apply color jitter and random grayscale.\n",
    "10. Convert to tensor.\n",
    "11. Normalize with ImageNet mean and std.\n",
    "12. Add batch dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFilter, ImageOps\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Tuple\n",
    "\n",
    "# Load model once\n",
    "class_model = # TO DO\n",
    "class_model.eval().to(device)\n",
    "\n",
    "# ImageNet labels\n",
    "url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
    "imagenet_classes = requests.get(url).text.strip().split(\"\\n\")\n",
    "\n",
    "# Preprocessing\n",
    "def preprocessing(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    12-step preprocessing.\n",
    "    Returns: tensor ready for EfficientNet-B4\n",
    "    \"\"\"\n",
    "    # 1. # TO DO\n",
    "\n",
    "    # 2. # TO DO\n",
    "    \n",
    "    # 3. # TO DO\n",
    "    \n",
    "    # 4. # TO DO\n",
    "    \n",
    "    # 5. # TO DO\n",
    "    \n",
    "    # 6. # TO DO\n",
    "    \n",
    "    # 7. # TO DO\n",
    "    \n",
    "    # 8. # TO DO\n",
    "    \n",
    "    # 9. # TO DO\n",
    "    \n",
    "    # 10. Convert to tensor\n",
    "    tensor = transforms.ToTensor()(img)\n",
    "\n",
    "    # 11. Advanced normalization: subtract mean and divide by std per channel\n",
    "    tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])(tensor)\n",
    "\n",
    "    # 12. Add batch dimension\n",
    "    return tensor.unsqueeze(0).to(device)\n",
    "\n",
    "# Classify\n",
    "def classify(img: Image.Image, topk: int = 5):\n",
    "    # First preprocess the input image, then feed it to the network\n",
    "    #TO DO\n",
    "    \n",
    "    probs = F.softmax(output[0], dim=0)\n",
    "    top_prob, top_idx = torch.topk(probs, topk)\n",
    "    top_prob = top_prob.cpu().numpy()\n",
    "    top_idx = top_idx.cpu().numpy()\n",
    "\n",
    "    tensor_img = preprocessed_image.squeeze().cpu()\n",
    "\n",
    "    # If tensor is normalized, undo normalization (example for ImageNet mean/std)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    tensor_img = tensor_img * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "    # Convert to numpy and transpose to HWC\n",
    "    tensor_img = tensor_img.numpy().transpose(1, 2, 0)\n",
    "\n",
    "    # Plot both original and preprocessed tensor\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(\"Input Image\")\n",
    "    ax1.axis('off')\n",
    "    ax2.imshow(tensor_img)\n",
    "    ax2.set_title(\"Preprocessed Image\")\n",
    "    ax2.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print top 5 prediction result for the preprocessed image\n",
    "    # TO DO\n",
    "\n",
    "# Evaluate the model one Image3, Image4, Image5, and Image6\n",
    "# TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Task 2 – Object Detection\n",
    "In this task, you are to use pretrained **R-CNN ResNet50-FPN-v2** model for object detection task.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Explain the functionality of the model briefly and indicate the dataset it is trained on.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Faster R-CNN\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "import torchvision.transforms as T\n",
    "\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "model.eval()\n",
    "model.to('cpu')  # ← crucial!\n",
    "\n",
    "transform = weights.transforms()\n",
    "\n",
    "def frcnn_detect(img):\n",
    "    # Complete the function to do the functionality of FRCNN for object detection\n",
    "    #TO DO\n",
    "\n",
    "# Feed Image1 and Image2 to the model and evaluate it\n",
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Task 3 – Semantic Segmentation\n",
    "\n",
    "In this task, you are to use the pretrained **DeepLabV3+ with ResNet50 backbone (torchvision)** model for a segemntation task.\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Load the model using the \"COCO_WITH_VOC_LABELS_V1\" weights.\n",
    "\n",
    "Do the required preprocessing\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = # TO DO\n",
    "seg_model.eval()\n",
    "seg_model.to(device)\n",
    "\n",
    "seg_preprocess = transforms.Compose([\n",
    "    # TO DO\n",
    "])\n",
    "\n",
    "# PASCAL VOC colors (21 classes)\n",
    "VOC_COLORS = np.array([\n",
    "    [0, 0, 0],      # background\n",
    "    [128, 0, 0],    # aeroplane\n",
    "    [0, 128, 0],    # bicycle\n",
    "    [128, 128, 0],  # bird\n",
    "    [0, 0, 128],    # boat\n",
    "    [128, 0, 128],  # bottle\n",
    "    [0, 128, 128],  # bus\n",
    "    [128, 128, 128],# car\n",
    "    [64, 0, 0],     # cat\n",
    "    [192, 0, 0],    # chair\n",
    "    [64, 128, 0],   # cow\n",
    "    [192, 128, 0],  # dining table\n",
    "    [64, 0, 128],   # dog\n",
    "    [192, 0, 128],  # horse\n",
    "    [64, 128, 128], # motorbike\n",
    "    [192, 128, 128],# person\n",
    "    [0, 64, 0],     # potted plant\n",
    "    [128, 64, 0],   # sheep\n",
    "    [0, 192, 0],    # sofa\n",
    "    [128, 192, 0],  # train\n",
    "    [0, 192, 128]   # tv/monitor\n",
    "])\n",
    "\n",
    "def segment_image(img):\n",
    "    input_tensor = seg_preprocess(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = seg_model(input_tensor)['out'][0]\n",
    "    pred = output.argmax(0).cpu().numpy()\n",
    "    \n",
    "    # Create colored mask\n",
    "    # TO DO\n",
    "    \n",
    "    # Overlay\n",
    "    # TO DO\n",
    "    \n",
    "    # Plot the orginal image and preprocessed version.\n",
    "    # TO DO\n",
    "    \n",
    "    return pred\n",
    "\n",
    "# Evaluate the model on Image2, Image3, and Image6\n",
    "# TO DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
